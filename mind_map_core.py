import os
import uvicorn
import google.generativeai as genai
import json
import numpy as np
import neo4j
from neo4j import GraphDatabase
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any
from pydantic import BaseModel 
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Path
from dotenv import load_dotenv
from fastapi import status

# .env íŒŒì¼ì—ì„œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
load_dotenv()

# Google API í‚¤ ì„¤ì • (Gemini APIìš©)
API_KEY = os.getenv("GOOGLE_API_KEY")
if not API_KEY:
    raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

# Neo4j í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
NEO4J_URI = os.getenv("NEO4J_URI")
NEO4J_USER = os.getenv("NEO4J_USER")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")
if not all([NEO4J_URI, NEO4J_USER, NEO4J_PASSWORD]):
    raise ValueError("Neo4j í™˜ê²½ ë³€ìˆ˜ê°€ ëª¨ë‘ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

# ì „ì—­ ë³€ìˆ˜ ì„¤ì •
embedding_model = None
neo4j_driver = None
MODEL_NAME = 'intfloat/multilingual-e5-large-instruct'

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì„œë²„ ì‹œì‘ ë° ì¢…ë£Œ ì‹œ ì´ë²¤íŠ¸ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €."""
    global embedding_model, neo4j_driver
    print("ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
    try:
        embedding_model = SentenceTransformer(MODEL_NAME)
        print("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
    except Exception as e:
        print(f"ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="ì„ë² ë”© ëª¨ë¸ ë¡œë“œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    print("Neo4j ë“œë¼ì´ë²„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    try:
        neo4j_driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))
        neo4j_driver.verify_connectivity()
        print("Neo4j ì—°ê²° ì„±ê³µ!")
        with neo4j_driver.session() as session:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (n:Keyword) REQUIRE n.name IS UNIQUE")
    except Exception as e:
        print(f"Neo4j ì—°ê²° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail="Neo4j ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    yield

    if neo4j_driver:
        neo4j_driver.close()
        print("Neo4j ì—°ê²° ì¢…ë£Œ.")

app = FastAPI(title="ë§ˆì¸ë“œë§µ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (Neo4j)", lifespan=lifespan)

# --- Pydantic ëª¨ë¸ ì •ì˜ ---
class QuestionRequest(BaseModel):
    question: str

class NodeDetailResponse(BaseModel):
    name: str
    description: str | None

# --- Gemini API í˜¸ì¶œ í•¨ìˆ˜ë“¤ (ë³€ê²½ ì—†ìŒ) ---
async def get_direct_answer_from_question(question: str) -> str:
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ ë‹µë³€í•©ë‹ˆë‹¤."""
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  ìƒì„¸í•˜ê²Œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”: {question}"
    try:
        response = await model.generate_content_async(prompt)
        return response.text
    except Exception as e:
        print(f"ì§ì ‘ ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

async def extract_keywords_from_question(question: str) -> List[str]:
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    prompt = (
        f"You are a highly specialized keyword extraction bot. Your sole function is to identify and extract core, foundational learning keywords from a given text.\n"
        f"Your output is strictly limited to a clean, comma-separated list of keywords. No other text, explanations, or punctuation are permitted.\n"
        f"**Extract keywords that are explicitly present in the provided text. Do not add any keywords that are not directly mentioned in the input text.**\n"
        f"Example 1:\nQuestion: TCP/IPê°€ ë­ì•¼? ì„¤ëª…í•´ì¤„ë˜?\nOutput: TCP, IP\n"
        f"Example 2:\nQuestion: ë§ˆì¼€íŒ…ì—ì„œ 4P ì „ëµì´ ë­”ì§€ ì•Œë ¤ì¤˜\nOutput: ë§ˆì¼€íŒ…, 4P ì „ëµ\n"
        f"Question: {question}\nOutput:"
    )
    try:
        response = await model.generate_content_async(prompt)
        return [keyword.strip() for keyword in response.text.split(',') if keyword.strip()]
    except Exception as e:
        print(f"í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []

async def get_additional_info_and_questions(keywords: List[str]) -> Dict[str, Any]:
    """Gemini APIë¥¼ ì‚¬ìš©í•˜ì—¬ 'ìƒˆë¡œìš´' í‚¤ì›Œë“œì— ëŒ€í•œ ì„¤ëª…ê³¼ í™•ì¥ ì§ˆë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not keywords:
        return {"answers": [], "expansion_questions": []}
    
    prompt = (
        f"Based on the following keywords: {', '.join(keywords)}, provide a concise explanation for each and generate related, follow-up questions to encourage further learning.\n"
        f"Format your response as a JSON object with two keys: 'answers' and 'expansion_questions'.\n"
        f"1. 'answers': A list of objects, each with a 'keyword' and a 'description'.\n"
        f"2. 'expansion_questions': A list containing a minimum of 1 and a maximum of 3 thought-provoking questions.\n\n"
        f"**IMPORTANT: All descriptions and questions must be written in Korean.**\n\n"
        f"Example:\n"
        f"{{\n"
        f"  \"answers\": [\n"
        f"    {{\"keyword\": \"TCP\", \"description\": \"ì „ì†¡ ì œì–´ í”„ë¡œí† ì½œìœ¼ë¡œ, ì‹ ë¢°ì„± ìˆëŠ” ë°ì´í„° ì „ì†¡ì„ ë³´ì¥í•©ë‹ˆë‹¤.\"}},\n"
        f"    {{\"keyword\": \"UDP\", \"description\": \"ì‚¬ìš©ì ë°ì´í„°ê·¸ë¨ í”„ë¡œí† ì½œìœ¼ë¡œ, ë¹ ë¥¸ ì†ë„ë¥¼ ìš°ì„ ì‹œí•©ë‹ˆë‹¤.\"}}\n"
        f"  ],\n"
        f"  \"expansion_questions\": [\n"
        f"    \"TCPì™€ UDPì˜ ë™ì‘ ê¸°ë°˜ì¸ OSI 7ê³„ì¸µì— ëŒ€í•´ ì„¤ëª…í•´ë³¼ ìˆ˜ ìˆë‚˜ìš”?\",\n"
        f"    \"TCPì˜ íë¦„ ì œì–´ì™€ í˜¼ì¡ ì œì–´ ë©”ì»¤ë‹ˆì¦˜ì€ ì–´ë–»ê²Œ ë™ì‘í•˜ë‚˜ìš”?\"\n"
        f"  ]\n"
        f"}}\n\n"
        f"Your output MUST be a valid JSON object. No extra text or explanations.\n"
        f"Keywords for generation: {', '.join(keywords)}\n"
        f"JSON output:"
    )
    try:
        response = await model.generate_content_async(prompt)
        json_text = response.text.replace("```json", "").replace("```", "").strip()
        return json.loads(json_text)
    except Exception as e:
        print(f"ìƒˆ í‚¤ì›Œë“œ ì •ë³´ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return {"answers": [], "expansion_questions": []}

# --- ë°ì´í„°ë² ì´ìŠ¤ ë° ë¡œì§ í•¨ìˆ˜ë“¤ (ë¡œì§ ë³€ê²½) ---
async def get_all_keywords_from_db() -> Dict[str, Dict[str, Any]]:
    """Neo4j DBì—ì„œ í‚¤ì›Œë“œ ì´ë¦„, ì„¤ëª…, ì„ë² ë”©ì„ í¬í•¨í•œ ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    query = "MATCH (n:Keyword) WHERE n.embedding IS NOT NULL RETURN n.name AS name, n.description AS description, n.embedding AS embedding"
    with neo4j_driver.session() as session:
        result = session.run(query)
        return {
            record["name"]: {"description": record["description"], "embedding": record["embedding"]}
            for record in result
        }

def find_strongest_connections(all_keywords: List[str], embeddings: np.ndarray, threshold: float) -> List[Dict[str, Any]]:
    """ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³ , ì‚¬ì´í´ì´ ì—†ëŠ” ìµœê°•ì˜ ì—°ê²°ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    class UnionFind:
        def __init__(self, items):
            self.parent = {item: item for item in items}
        def find(self, item):
            if self.parent[item] == item: return item
            self.parent[item] = self.find(self.parent[item])
            return self.parent[item]
        def union(self, item1, item2):
            root1, root2 = self.find(item1), self.find(item2)
            if root1 != root2:
                self.parent[root2] = root1
                return True
            return False

    similarity_matrix = cosine_similarity(embeddings)
    all_connections = []
    for i in range(len(all_keywords)):
        for j in range(i + 1, len(all_keywords)):
            if (similarity := similarity_matrix[i, j]) >= threshold:
                all_connections.append({"source": all_keywords[i], "target": all_keywords[j], "similarity": float(similarity)})
    
    all_connections.sort(key=lambda x: x['similarity'], reverse=True)
    uf = UnionFind(all_keywords)
    return [conn for conn in all_connections if uf.union(conn['source'], conn['target'])]

async def save_to_neo4j(new_keywords_data: List[Dict[str, Any]], links: List[Dict[str, Any]]):
    """ìƒˆë¡œìš´ í‚¤ì›Œë“œ(ë…¸ë“œ+ì„ë² ë”©)ì™€ ë§í¬(ê´€ê³„)ë¥¼ Neo4jì— ì €ì¥í•©ë‹ˆë‹¤."""
    with neo4j_driver.session() as session:
        # 1. ìƒˆë¡œìš´ ë…¸ë“œì™€ ì„ë² ë”© ì €ì¥
        for kw_data in new_keywords_data:
            session.run(
                "MERGE (k:Keyword {name: $name}) "
                "SET k.description = $description, k.embedding = $embedding",
                name=kw_data['keyword'],
                description=kw_data['description'],
                embedding=kw_data['embedding']
            )
        # 2. ìƒˆë¡œìš´ ê´€ê³„ ì €ì¥
        for link in links:
            session.run("MATCH (a:Keyword {name: $source}) MATCH (b:Keyword {name: $target}) "
                        "MERGE (a)-[r:RELATED_TO]-(b) SET r.similarity = $similarity",
                        source=link['source'], target=link['target'], similarity=link['similarity'])
    if new_keywords_data or links:
        print("Neo4jì— ë°ì´í„° ì €ì¥ ì™„ë£Œ!")


@app.delete("/delete-all", status_code=status.HTTP_204_NO_CONTENT)
async def delete_all_data():
    """
    Neo4j ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ë…¸ë“œì™€ ê´€ê³„ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤.
    """
    try:
        with neo4j_driver.session() as session:
            session.run("MATCH (n) DETACH DELETE n")
        print("ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return
    except Exception as e:
        print(f"ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail="ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
    
@app.post("/generate_mind_map_data")
async def process_and_generate_mind_map_neo4j(request: QuestionRequest):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ë§ˆì¸ë“œë§µ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  Neo4jì— ì €ì¥í•©ë‹ˆë‹¤."""
    # 1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ ì§ì ‘ì ì¸ ë‹µë³€ì„ ë¨¼ì € ìƒì„±í•©ë‹ˆë‹¤.
    direct_answer = await get_direct_answer_from_question(request.question)

    # 2. ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    extracted_keywords = await extract_keywords_from_question(request.question)
    if not extracted_keywords:
        raise HTTPException(status_code=400, detail="í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # 3. DBì—ì„œ ê¸°ì¡´ í‚¤ì›Œë“œ ë°ì´í„°(ì„¤ëª…, ì„ë² ë”© í¬í•¨)ë¥¼ ê°€ì ¸ì˜¤ê³  ìƒˆë¡œìš´ í‚¤ì›Œë“œë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    all_keywords_data_in_db = await get_all_keywords_from_db()
    existing_keyword_names = set(all_keywords_data_in_db.keys())
    new_keywords_to_process = [kw for kw in extracted_keywords if kw not in existing_keyword_names]

    # 4. ìƒˆë¡œìš´ í‚¤ì›Œë“œì— ëŒ€í•œ ì„¤ëª…/ì§ˆë¬¸ ìƒì„± ë° ì„ë² ë”© ê³„ì‚°
    new_keyword_info = await get_additional_info_and_questions(new_keywords_to_process)
    
    new_keywords_to_save = []
    new_keyword_embeddings_map = {}
    if new_keywords_to_process:
        # ìƒˆë¡œ ì¶”ê°€í•  í‚¤ì›Œë“œì˜ ì„ë² ë”©ë§Œ ê³„ì‚°í•©ë‹ˆë‹¤.
        new_embeddings_list = embedding_model.encode(new_keywords_to_process).tolist()
        descriptions_map = {ans['keyword']: ans['description'] for ans in new_keyword_info.get('answers', [])}

        for i, keyword in enumerate(new_keywords_to_process):
            embedding = new_embeddings_list[i]
            new_keyword_embeddings_map[keyword] = embedding
            new_keywords_to_save.append({
                "keyword": keyword,
                "description": descriptions_map.get(keyword, "ì„¤ëª… ì—†ìŒ"),
                "embedding": embedding,
            })
    
    # 5. ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´ ì „ì²´ í‚¤ì›Œë“œ ë° ì„ë² ë”© ëª©ë¡ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.
    all_unique_keywords = list(existing_keyword_names.union(set(extracted_keywords)))
    all_embeddings = []
    for keyword in all_unique_keywords:
        if keyword in existing_keyword_names:
            all_embeddings.append(all_keywords_data_in_db[keyword]["embedding"])
        else:
            all_embeddings.append(new_keyword_embeddings_map[keyword])

    # 6. í‚¤ì›Œë“œ ê°„ì˜ ì—°ê²°(ë§í¬)ì„ ìƒì„±í•˜ê³  DBì— ì €ì¥í•©ë‹ˆë‹¤.
    links_to_save = find_strongest_connections(all_unique_keywords, np.array(all_embeddings), 0.4)
    await save_to_neo4j(new_keywords_to_save, links_to_save)
    
    # 7. ìµœì¢… ë°˜í™˜í•  ì‘ë‹µì„ ì¡°ë¦½í•©ë‹ˆë‹¤.
    full_answer = ""
    if direct_answer and direct_answer != "ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.":
        full_answer += f"{direct_answer}\n\n---\n\n"

    # ì „ì²´ í‚¤ì›Œë“œì˜ ì„¤ëª… ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
    final_descriptions_map = {name: data["description"] for name, data in all_keywords_data_in_db.items()}
    for item in new_keywords_to_save:
        final_descriptions_map[item['keyword']] = item['description']

    if new_keyword_info.get('answers'):
        full_answer += "### ìƒˆë¡­ê²Œ ì¶”ê°€ëœ í‚¤ì›Œë“œ ì„¤ëª…\n\n"
        for ans in new_keyword_info['answers']:
            full_answer += f"**{ans['keyword']}:** {ans['description']}\n\n"
    
    expansion_questions = new_keyword_info.get('expansion_questions', [])
    if expansion_questions:
        full_answer += "\n---\n\n"
        full_answer += "### ë” ê³µë¶€í•´ë³¼ê¹Œìš”? ğŸ¤”\n\n"
        for q in expansion_questions[:3]:
            full_answer += f"- {q}\n"
    
    final_nodes = [
        {"id": kw, "name": kw, "description": final_descriptions_map.get(kw, "ì„¤ëª… ì—†ìŒ")}
        for kw in all_unique_keywords
    ]

    final_response = {
        "answer": full_answer.strip(),
        "nodes": final_nodes,
        "links": links_to_save
    }
    
    print("ë§ˆì¸ë“œë§µ ë°ì´í„° ìƒì„± ë° ì €ì¥ ì™„ë£Œ. í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤.")
    return final_response

@app.get("/node/{node_name}", response_model=NodeDetailResponse)
async def get_node_details(node_name: str = Path(..., description="ì •ë³´ë¥¼ ì¡°íšŒí•  ë…¸ë“œì˜ ì´ë¦„")):
    """íŠ¹ì • ë…¸ë“œì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    query = "MATCH (n:Keyword {name: $name}) RETURN n.name AS name, n.description AS description"
    try:
        with neo4j_driver.session() as session:
            result = session.run(query, name=node_name).single()
            if not result:
                raise HTTPException(status_code=404, detail="í•´ë‹¹ ì´ë¦„ì˜ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            return {
                "name": result["name"],
                "description": result["description"]
            }
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"ë…¸ë“œ ìƒì„¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise HTTPException(status_code=500, detail="ì„œë²„ ë‚´ë¶€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000, reload=True)